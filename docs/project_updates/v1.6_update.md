# 📢 CORI Update – v1.6: July 1, 2025

🏗️ **COMPLETE SYSTEM REFACTOR!** CORI has undergone a **massive architectural overhaul** with professional-grade modular design, enhanced sensor fusion capabilities, and **real-time color detection driving autonomous head movement**. The entire codebase has been restructured for scalability, maintainability, and production-ready deployment.

**System Status:** ✅ FULLY OPERATIONAL  
**Integration Level:** Professional Architecture with Real-Time Sensor Fusion  

<div align="center">
  <table>
    <tr>
      <td width="50%" align="center">
        <img src="../../assets/gifs/CORI_Physical_Detection_to_Joint_Pos.gif" height="400px"/>
        <br><b>Physical Detection → Joint Movement</b><br>
        <em>Physical Camera Drives Virtual Head</em>
      </td>
      <td width="50%" align="center">
        <img src="../../assets/gifs/CORI_Digital_Detection_to_Joint_Pos.gif" height="400px"/>
        <br><b>Digital Detection → Joint Movement</b><br>
        <em>Virtual Head Looks at Virtual Objects</em>
      </td>
    </tr>
  </table>
</div>

## ✅ Major Achievements

### 🏗️ Complete Architectural Refactor
- ✅ **Modular ROS 2 Package Structure:** Professional workspace organization ([File Structure Reference](../File_Structure.md))
- ✅ **Separated Concerns:** Independent packages for control, vision, simulation, and core utilities
- ✅ **Legacy Preservation:** Previous codebase available in `legacy` branch for reference
- ✅ **Production-Ready Architecture:** Scalable design supporting future expansion

### 🧠 Real-Time Sensor Fusion
- ✅ **Physical-to-Virtual Integration:** Real camera detection directly controls robot head movement
- ✅ **Dual Detection Modes:** Both physical camera and virtual world detection supported
- ✅ **Spatial Database Integration:** Head moves to remembered locations based on color history
- ✅ **Confidence-Driven Movement:** Head position accuracy improves with learning

### 🎯 Enhanced Computer Vision Pipeline
- ✅ **Production Color Detector:** Robust HSV-based classification with throttling
- ✅ **Multi-Camera Support:** Seamless switching between physical and virtual cameras
- ✅ **Stable Detection:** Anti-jitter filtering prevents rapid head movement changes
- ✅ **Professional Logging:** Clean terminal output with color-coded status messages

### 🔧 Advanced Robot Control
- ✅ **Precise Joint Control:** Direct publishing to `/model/cori/joint/head_joint/cmd_pos`
- ✅ **Smooth Movement:** PID-controlled head positioning with proper physics
- ✅ **Safety Constraints:** Joint limits and collision avoidance
- ✅ **Error Recovery:** Robust handling of movement failures and edge cases

---

## 🏗️ New Modular Architecture

### Package Structure Overview
Refer to [**File Structure Documentation**](../File_Structure.md) for complete details:

```
cori_ws/
├── src/
│   ├── cori_control/      # Low-level joint control
│   ├── cori_core/         # Database & utilities
│   ├── cori_description/  # URDF, meshes, worlds
│   ├── cori_gui/          # User interfaces
│   ├── cori_simulation/   # Behavior modeling
│   ├── cori_tools/        # Development utilities
│   └── cori_vision/       # Computer vision pipeline
└── shared/                # Common assets & database
```

### Migration Notes
- **Legacy Code:** Previous v1.5 implementation preserved in `legacy` branch
- **Breaking Changes:** New package structure requires updated launch procedures
- **Improved APIs:** Cleaner interfaces between modules
- **Enhanced Documentation:** Each package includes comprehensive README files

---

## 🔮 Future Implementation: FSM-Driven Behavioral Intelligence

### Finite State Machine Integration
The [**Finite State Machine Documentation**](../FSM.md) outlines CORI's behavioral framework for transitioning from reactive color detection to intelligent decision-making:

#### **Mode 1: Intelligent Color Detection & Sorting**
**State Flow:** `ColorDetection` → `HeadTurn` → `CheckConfidence` → `Decision Branch`

- **Learning Phase:** Asks questions, builds knowledge base
- **Tentative Phase:** "I think this goes in colors - agree?"  
- **Confident Phase:** Autonomous sorting with occasional validation
- **Adaptive Questioning:** Frequency decreases as confidence increases

#### **Mode 2: Spatial Object Finding**
**State Flow:** `FindItem` → `QueryDatabase` → `CheckPosition` → `ConfirmItem`

- **Natural Language Queries:** "Find my red shirt" → head movement to last known location
- **Spatial Memory Integration:** Combines color detection with position history
- **Interactive Confirmation:** "Is this the red shirt you were looking for?"
- **Learning from Misses:** Updates database when objects move

#### **Mode 3: Analytics & Optimization**
**State Flow:** `HamperBreakdown` → `SortOptions` → `DisplayResults`

- **Performance Analytics:** Sorting success rates, confidence progression
- **Pattern Recognition:** Identifies user preference trends over time
- **Optimization Suggestions:** Recommends workflow improvements

### Behavioral Intelligence Features
- **Confidence-Based Decision Making:** FSM transitions based on accumulated knowledge
- **Progressive Learning:** System becomes more autonomous as expertise grows
- **Error Recovery:** Graceful handling of misidentifications and edge cases
- **Multi-User Adaptation:** Different behavioral patterns for different users

---

## 🎯 Latest Feature: Real-Time Color-Driven Head Movement

### Physical Detection Mode
**Flow:** `Real Camera → OpenCV Processing → Color Classification → Head Joint Command`
- Detects colors in physical environment using webcam
- Instantly moves head to corresponding spatial database position
- Learns and refines spatial memory through repeated observations

### Virtual Detection Mode  
**Flow:** `Virtual Camera → Gazebo Simulation → Color Analysis → Joint Movement`
- Processes virtual world objects in Gazebo simulation
- Demonstrates same sensor fusion pipeline in controlled environment
- Perfect for testing and development without physical hardware

### Sensor Fusion Integration
- **Seamless Switching:** Toggle between physical and virtual detection modes
- **Consistent Behavior:** Same movement patterns regardless of input source
- **Learning Transfer:** Spatial memory applies across both detection modes
- **Professional Integration:** Clean ROS 2 message passing between all components

---

## 🚀 Technical Implementation Highlights

### Color-to-Movement Mapping
```python
color_angles = {
    'red':    -0.62,  # Left
    'orange': -0.45,  # Mid-left  
    'yellow': -0.23,  # Slight left
    'green':   0.0,   # Center
    'blue':    0.23,  # Slight right
    'purple':  0.45,  # Mid-right
    'black':   0.76   # Far right
}
```

### Professional Error Handling
- **Graceful Degradation:** System continues operating even with component failures
- **Resource Management:** Proper cleanup of camera, simulation, and ROS resources
- **Debug Logging:** Comprehensive status tracking for development and deployment

### Performance Optimization
- **Throttled Detection:** 0.5-second hold times prevent jittery movement
- **Efficient Processing:** Optimized OpenCV pipeline for real-time operation
- **Memory Management:** Persistent database with intelligent caching

---

## 💫 Why This Refactor Matters

### **For Development:**
- **Faster Iteration:** Modular architecture enables independent package development
- **Better Testing:** Isolated components simplify unit and integration testing  
- **Cleaner Code:** Professional structure reduces technical debt

### **For Features:**
- **Sensor Fusion:** Real-time physical-virtual integration opens new possibilities
- **Scalability:** Architecture ready for multi-domain expansion beyond laundry
- **Reliability:** Production-grade error handling and resource management

### **For Future:**
- **Research Platform:** FSM framework supports complex behavioral modeling
- **Commercial Viability:** Professional codebase suitable for product development
- **Innovation Foundation:** Modular design enables rapid prototyping of new capabilities

---

## 🏆 Achievement Summary

**v1.6 represents a quantum leap in CORI's development:**

- **Complete system refactor** with professional architecture
- **Real-time sensor fusion** connecting physical and virtual worlds
- **Production-ready codebase** with comprehensive error handling
- **Scalable foundation** for future multi-domain intelligence

### **Success Metrics:**
- **6 Independent Packages** with clean interfaces
- **Real-Time Performance** with <100ms detection-to-movement latency
- **Robust Operation** with comprehensive error recovery
- **Future-Ready Architecture** supporting FSM behavioral framework

---

🎯 **Milestone Achieved:** CORI has evolved from a prototype to a **professional robotics platform** with real-time sensor fusion capabilities and a foundation for truly intelligent household assistance.

## 🔥 What's Next: The Physical Intelligence Evolution

The refactored architecture and sensor fusion pipeline set the stage for **Phase 3: Complete Autonomous Sorting** - a systematic progression toward full physical intelligence through three major development sprints.

### **v1.7: EKF-Powered Autonomous Navigation** 🧭
**Target:** Complete autonomous mobility with Extended Kalman Filter state estimation

**Locomotion + EKF Integration:**
- **Dynamic Walking:** Stable bipedal locomotion with EKF-guided balance control
- **SLAM Navigation:** Simultaneous localization and mapping with uncertainty quantification
- **Multi-Room Autonomy:** Independent navigation between laundry areas, bedrooms, closets
- **Obstacle Avoidance:** Real-time path replanning with predictive collision detection

**EKF State Estimation:**
- **Multi-Sensor Fusion:** Visual odometry, IMU, joint encoders, and camera data
- **Predictive Tracking:** Anticipate object and self-position with uncertainty bounds
- **Adaptive Filtering:** Dynamic noise models for different terrains and lighting
- **Persistent Mapping:** Long-term spatial memory with confidence-weighted updates

**Technical Implementation:**
```python
# EKF State: [robot_x, robot_y, robot_theta, velocity, object_map...]
while autonomous_operation():
    # Predict robot state during movement
    predicted_state = ekf.predict(locomotion_commands, dt)
    
    # Update with sensor measurements
    measurements = [camera_detection, imu_data, joint_encoders, visual_odometry]
    corrected_state = ekf.update(measurements)
    
    # Plan next movement with uncertainty
    target_object = select_next_target(corrected_state.object_map)
    safe_path = path_planner.navigate_with_uncertainty(corrected_state.covariance)
    execute_movement(safe_path)
```

**Autonomous Capabilities:**
- **Independent Operation:** No human positioning or guidance required
- **Terrain Adaptation:** Walking on carpet, hardwood, tile with gait adjustment
- **Load Compensation:** Balance control when carrying objects of varying weights
- **Energy Optimization:** Efficient routes that maximize operational time

---

### **v1.8: Gripper Sprint** 🤖
**Target:** Complete manipulation pipeline from detection to placement

**Physical Integration:**
- **Full Arm Kinematics:** 6-DOF arm control with inverse kinematics solver
- **Adaptive Grasping:** Force-feedback gripper with object-specific grip strategies
- **Collision Avoidance:** Real-time path planning with obstacle detection
- **Pick-and-Place Pipeline:** Coordinated vision → approach → grasp → transport → release

**Gripper Intelligence Features:**
- **Object Classification:** Different grip strategies for shirts vs. pants vs. socks
- **Force Control:** Delicate handling prevents fabric damage
- **Failure Recovery:** Graceful handling of dropped objects and grip failures
- **Learning Optimization:** Grip success rate improvement over time

**Integration with EKF:**
- **Precise Positioning:** EKF state estimates guide gripper approach vectors
- **Dynamic Replanning:** Real-time trajectory adjustment based on updated object positions
- **Uncertainty Handling:** Grip confidence based on EKF spatial uncertainty

**Technical Milestones:**
```python
# Complete manipulation pipeline
object_pose = ekf.get_object_estimate(detected_color)
trajectory = motion_planner.plan_approach(current_pose, object_pose)
gripper.execute_grasp(object_properties, force_threshold)
delivery_path = path_planner.navigate_to_hamper(object_classification)
gripper.controlled_release(hamper_position)
```

---

### **v1.9: Locomotion Sprint** 🚶
**Target:** Full-body mobility and autonomous navigation

**Locomotion Capabilities:**
- **Dynamic Walking:** Stable bipedal locomotion with balance control
- **Load Compensation:** Gait adjustment when carrying objects of varying weights
- **Terrain Adaptation:** Walking on different household surfaces (carpet, hardwood, tile)
- **Navigation Intelligence:** SLAM-based mapping with dynamic obstacle avoidance

**Advanced Mobility Features:**
- **Multi-Room Operation:** Autonomous navigation between laundry room, bedrooms, closets
- **Stair Climbing:** Vertical mobility for multi-story homes (stretch goal)
- **Furniture Navigation:** Maneuvering around chairs, tables, and household obstacles
- **Energy Optimization:** Efficient movement patterns to maximize battery life

**EKF-Guided Locomotion:**
- **Localization:** Continuous pose estimation during movement
- **Path Optimization:** EKF uncertainty guides exploration vs. exploitation decisions
- **Object Tracking:** Maintain awareness of object locations while moving
- **Dynamic Replanning:** Real-time route adjustment based on new discoveries

**Complete System Integration:**
```python
# Full autonomous sorting cycle
while household_items_detected():
    object_location = ekf_localize_and_track()
    navigate_to_object(path_planner.optimize_route())
    manipulate_object(gripper_controller.adaptive_grasp())
    classify_and_sort(learned_preferences)
    navigate_to_hamper(locomotion_controller.stable_walk())
    update_spatial_memory(ekf.persistent_mapping())
```

---

### **Phase 3 Complete: Autonomous Household Intelligence** 🏠

**The Ultimate Integration:**
By v2.0, CORI will demonstrate complete autonomous sorting:

1. **Discovery Phase:** Walk through house, use EKF to map and locate all laundry items
2. **Planning Phase:** Optimize collection and sorting routes based on learned preferences  
3. **Execution Phase:** Autonomous pickup, classification, and delivery to appropriate hampers
4. **Learning Phase:** Continuously refine spatial maps, grip strategies, and user preferences

**System Capabilities:**
- **Multi-Room Autonomy:** Complete household coverage with persistent mapping
- **Intelligent Prioritization:** Understands urgency (work clothes vs. weekend items)
- **Adaptive Behavior:** Adjusts to household changes (furniture moved, new rooms)
- **Cooperative Intelligence:** Seamless human-robot collaboration when needed

---

## 🎯 Why This Progression Matters

### **EKF Foundation (v1.7):**
- **Spatial Intelligence:** Transforms CORI from reactive to predictive
- **Professional Standards:** Industry-grade state estimation
- **Manipulation Enabler:** Precise localization required for gripper success

### **Gripper Mastery (v1.8):**
- **Physical Capability:** Moves from observation to action
- **Task Completion:** First complete pick-and-place operations
- **Learning Platform:** Manipulation success drives intelligence improvements

### **Locomotion Freedom (v1.9):**
- **Autonomous Operation:** Complete independence from human positioning
- **Household Scale:** Operates at the scale of real homes
- **True Utility:** Performs actual household labor autonomously

---

🎯 **Vision Realized:** By v2.0, CORI evolves from a stationary color detector to a **fully autonomous household intelligence** capable of independent laundry sorting across multi-room environments.

*This isn't just robot development - it's the systematic construction of cooperative intelligence that can truly partner with humans in managing household tasks.*****
